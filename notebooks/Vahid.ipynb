{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf57764c-a45a-4af1-809e-b5e87db9a3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ca0425e-c81f-417d-bd7a-056c374d3029",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input: int, output: int, a_function):\n",
    "        self._weights = np.random.rand(output, input)\n",
    "        self._bias = np.random.rand(output, 1)\n",
    "        self._activation = a_function\n",
    "\n",
    "    def forward(self, input: np.ndarray):\n",
    "        a = self._weights @ input+self._bias\n",
    "        o = self._activation(a)\n",
    "        return o\n",
    "    def __str__(self):\n",
    "        return f\"Layer of {len(self._weights)} neurons\"\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_sizes: list,\n",
    "        output_size: int,\n",
    "        activation_function,\n",
    "        final_activation=True,\n",
    "    ):\n",
    "        if not isinstance(activation_function, list):\n",
    "            activation_function=[activation_function for _ in range(len(hidden_sizes)+2)]\n",
    "        sizes=[input_size]+hidden_sizes+[output_size]\n",
    "        self._layers=[]\n",
    "        for i in range(len(sizes)-1):\n",
    "            self._layers.append(Layer(sizes[i], sizes[i+1], activation_function[i]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self._layers[0].forward(x)\n",
    "        for i in range(1, len(self._layers)):\n",
    "            z = self._layers[i].forward(z)\n",
    "        return z\n",
    "        # \"\"\"\n",
    "        # X: input with shape (input_size, batch_size) ← mini-batch\n",
    "        # Output: y_hat with shape (output_size, batch_size)\n",
    "        # \"\"\"\n",
    "        # # 1) Add bias neuron to input layer: (input_size+1, batch_size)\n",
    "        # Xb = self._append_bias_row(X)\n",
    "\n",
    "        # # 2) Pre-activation of hidden layer: z1 = W1 · Xb\n",
    "        # z1 = self.W1 @ Xb\n",
    "\n",
    "        # # 3) Hidden layer activation: a1 = sigmoid(z1) → (hidden_size, batch_size)\n",
    "        # a1 = self.sigmoid(z1)\n",
    "\n",
    "        # # 4) Add bias neuron to a1: (hidden_size+1, batch_size)\n",
    "        # a1b = self._append_bias_row(a1)\n",
    "\n",
    "        # # 5) Pre-activation of output layer: z2 = W2 · a1b\n",
    "        # z2 = self.W2 @ a1b\n",
    "\n",
    "        # # 6) Output activation: a2 = sigmoid(z2) → (output_size, batch_size)\n",
    "        # a2 = self.sigmoid(z2)\n",
    "\n",
    "        # # Save for backpropagation\n",
    "        # self.cache = {\n",
    "        #     \"X\": X, \"Xb\": Xb,\n",
    "        #     \"z1\": z1, \"a1\": a1, \"a1b\": a1b,\n",
    "        #     \"z2\": z2, \"a2\": a2\n",
    "        # }\n",
    "        # return a2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389a1a06-bba5-4104-aded-55e704487658",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "944ea7a6-fc6d-48ed-bec6-6fcb7d3675b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.95532288]\n",
      " [-0.42093227]\n",
      " [-0.20349792]\n",
      " [-0.81083174]\n",
      " [-0.87235776]\n",
      " [-0.30812798]\n",
      " [ 0.92433168]\n",
      " [-0.97429069]\n",
      " [ 0.99948213]\n",
      " [ 0.46790707]]\n",
      "[[ 0.99895829 -0.97024588  0.95322357 -0.98609469 -0.64859183  0.99831752\n",
      "   0.45412877 -0.70005027 -0.72208131  0.34457931]\n",
      " [ 0.99765983  0.57349206  0.15735826 -0.68470696 -0.99938447  0.89559478\n",
      "   0.97860336  0.55534689 -0.05331321  0.16210834]\n",
      " [ 0.76838604 -0.11969727 -0.08978427 -0.83030519 -0.08286167 -0.81564909\n",
      "   0.92143629  0.25000883  0.90203579  0.76704666]\n",
      " [-0.97580648  0.42907293 -0.76685355  0.10641305 -0.53946174  0.11725587\n",
      "   0.40701228  0.84448515  0.09824011  0.93094159]\n",
      " [ 0.98500047 -0.05913097 -0.19019918  0.60905772 -0.97462232  0.51927884\n",
      "   0.12602272 -0.80150031 -0.97627526  0.92737517]\n",
      " [ 0.99252796 -0.52726814  0.89908762  0.89219433 -0.54756174  0.53948613\n",
      "   0.63569933  0.24797926 -0.99999371  0.76199531]\n",
      " [ 0.76535854  0.4474374  -0.10911285  0.42924518  0.54111399  0.71304532\n",
      "   0.97266845  0.72975971 -0.83579736  0.23674275]\n",
      " [ 0.26549555  0.59016798  0.8646655  -0.3648548  -0.99690984  0.7294006\n",
      "  -0.57679467  0.99297153  0.76485495  0.95135745]\n",
      " [ 0.02326926  0.9669724  -0.87448117  0.26293077 -0.99797064  0.87671556\n",
      "   0.99975692  0.30378405  0.99069517  0.83173087]\n",
      " [-0.76742931 -0.66052925  0.5717968  -0.99818937  0.12760745  0.73465302\n",
      "   0.7902598   0.98012453 -0.10833825  0.35589632]]\n"
     ]
    }
   ],
   "source": [
    "input_size, hidden_size, output_size = 784, 30, 10\n",
    "B = 16  # batch size\n",
    "\n",
    "nn = NeuralNetwork(784, [30], 10, np.sin)  # ← مقیاس خارج از منابع\n",
    "print(nn.forward(np.random.rand(784, 1)))\n",
    "print(nn.forward(np.random.rand(784, 10))) #seems to work\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
