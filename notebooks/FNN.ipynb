{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7563926b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3ebd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self):\n",
    "        self.train, self.valid, self.test = self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Load MNIST train, validation, and test data.\"\"\"\n",
    "        # For notebooks, use relative path to parent directory\n",
    "        data_path = '../mnist.pkl'\n",
    "        with open(data_path, 'rb') as f:\n",
    "            train, valid, test = pickle.load(f, encoding='latin-1')\n",
    "        return train, valid, test\n",
    "    \n",
    "    def get_data_information(self):\n",
    "        \"\"\"Return the shape of the datasets, their data types, unique labels and data range.\"\"\"\n",
    "        return {\n",
    "            'train': (self.train[0].shape, self.train[1].shape),\n",
    "            'train_data_type': type(self.train[0]),\n",
    "            'valid': (self.valid[0].shape, self.valid[1].shape),\n",
    "            'valid_data_type': type(self.valid[0]),\n",
    "            'test': (self.test[0].shape, self.test[1].shape),\n",
    "            'test_data_type': type(self.test[0]),\n",
    "            'unique_labels': set(self.train[1]),\n",
    "            'training_data_range': (self.train[0].min(), self.train[0].max())\n",
    "        }\n",
    "    \n",
    "    def get_train_data(self):\n",
    "        \"\"\"Return training data and labels.\"\"\"\n",
    "        return self.train\n",
    "\n",
    "    def get_valid_data(self):\n",
    "        \"\"\"Return validation data and labels.\"\"\"\n",
    "        return self.valid\n",
    "\n",
    "    def get_test_data(self):\n",
    "        \"\"\"Return test data and labels.\"\"\"\n",
    "        return self.test\n",
    "    \n",
    "    def get_all_data(self):\n",
    "        \"\"\"Return all datasets.\"\"\"\n",
    "        return self.train, self.valid, self.test\n",
    "    \n",
    "    def print_data_summary(self):\n",
    "        \"\"\"Print a summary of the datasets.\"\"\"\n",
    "        info = self.get_data_information()\n",
    "        print(\"Data Summary:\")\n",
    "        print(f\"Training set: {info['train'][0]} samples, Labels: {info['train'][1]}\")\n",
    "        print(f\"Validation set: {info['valid'][0]} samples, Labels: {info['valid'][1]}\")\n",
    "        print(f\"Test set: {info['test'][0]} samples, Labels: {info['test'][1]}\")\n",
    "        print(f\"Unique labels in training set: {info['unique_labels']}\")\n",
    "        print(f\"Training data range: {info['training_data_range']}\")\n",
    "    \n",
    "    def draw_sample(self, data, labels, index=None):\n",
    "        \"\"\"Draw a sample image from the dataset.\n",
    "        \n",
    "        Args:\n",
    "            data: Image data array (samples, height, width) or (samples, pixels)\n",
    "            labels: Label array\n",
    "            index: Index of image to draw (random if None)\n",
    "        \"\"\"\n",
    "        if index is None:\n",
    "            index = np.random.randint(0, len(data))\n",
    "        \n",
    "        image = data[index]\n",
    "        label = labels[index]\n",
    "        \n",
    "        # Reshape if flattened (784 pixels -> 28x28)\n",
    "        if len(image.shape) == 1:\n",
    "            image = image.reshape(28, 28)\n",
    "        \n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.title(f'Label: {label}')\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf332a0",
   "metadata": {},
   "source": [
    "# Data example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4be7620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Summary:\n",
      "Training set: (50000, 784) samples, Labels: (50000,)\n",
      "Validation set: (10000, 784) samples, Labels: (10000,)\n",
      "Test set: (10000, 784) samples, Labels: (10000,)\n",
      "Unique labels in training set: {np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9)}\n",
      "Training data range: (np.float32(0.0), np.float32(0.99609375))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test the DataLoader\n",
    "loader = DataLoader()\n",
    "loader.print_data_summary()\n",
    "train_data, train_labels = loader.get_train_data()\n",
    "loader.draw_sample(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87505dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input: int, output: int, a_function):\n",
    "        self._weights = np.random.rand(output, input)\n",
    "        self._bias = np.random.rand(output, 1)\n",
    "        self._activation = a_function\n",
    "\n",
    "    def forward(self, input: np.ndarray):\n",
    "        a = self._weights @ input+self._bias\n",
    "        o = self._activation(a)\n",
    "        return o\n",
    "    def __str__(self):\n",
    "        return f\"Layer of {len(self._weights)} neurons\"\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_sizes: list,\n",
    "        output_size: int,\n",
    "        activation_function,\n",
    "        final_activation=True,\n",
    "    ):\n",
    "        if not isinstance(activation_function, list):\n",
    "            activation_function=[activation_function for _ in range(len(hidden_sizes)+2)]\n",
    "        sizes=[input_size]+hidden_sizes+[output_size]\n",
    "        self._layers=[]\n",
    "        for i in range(len(sizes)-1):\n",
    "            self._layers.append(Layer(sizes[i], sizes[i+1], activation_function[i]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self._layers[0].forward(x)\n",
    "        for i in range(1, len(self._layers)):\n",
    "            z = self._layers[i].forward(z)\n",
    "        return z\n",
    "        # \"\"\"\n",
    "        # X: input with shape (input_size, batch_size) ← mini-batch\n",
    "        # Output: y_hat with shape (output_size, batch_size)\n",
    "        # \"\"\"\n",
    "        # # 1) Add bias neuron to input layer: (input_size+1, batch_size)\n",
    "        # Xb = self._append_bias_row(X)\n",
    "\n",
    "        # # 2) Pre-activation of hidden layer: z1 = W1 · Xb\n",
    "        # z1 = self.W1 @ Xb\n",
    "\n",
    "        # # 3) Hidden layer activation: a1 = sigmoid(z1) → (hidden_size, batch_size)\n",
    "        # a1 = self.sigmoid(z1)\n",
    "\n",
    "        # # 4) Add bias neuron to a1: (hidden_size+1, batch_size)\n",
    "        # a1b = self._append_bias_row(a1)\n",
    "\n",
    "        # # 5) Pre-activation of output layer: z2 = W2 · a1b\n",
    "        # z2 = self.W2 @ a1b\n",
    "\n",
    "        # # 6) Output activation: a2 = sigmoid(z2) → (output_size, batch_size)\n",
    "        # a2 = self.sigmoid(z2)\n",
    "\n",
    "        # # Save for backpropagation\n",
    "        # self.cache = {\n",
    "        #     \"X\": X, \"Xb\": Xb,\n",
    "        #     \"z1\": z1, \"a1\": a1, \"a1b\": a1b,\n",
    "        #     \"z2\": z2, \"a2\": a2\n",
    "        # }\n",
    "        # return a2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf64abd9",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76b6fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.95532288]\n",
      " [-0.42093227]\n",
      " [-0.20349792]\n",
      " [-0.81083174]\n",
      " [-0.87235776]\n",
      " [-0.30812798]\n",
      " [ 0.92433168]\n",
      " [-0.97429069]\n",
      " [ 0.99948213]\n",
      " [ 0.46790707]]\n",
      "[[ 0.99895829 -0.97024588  0.95322357 -0.98609469 -0.64859183  0.99831752\n",
      "   0.45412877 -0.70005027 -0.72208131  0.34457931]\n",
      " [ 0.99765983  0.57349206  0.15735826 -0.68470696 -0.99938447  0.89559478\n",
      "   0.97860336  0.55534689 -0.05331321  0.16210834]\n",
      " [ 0.76838604 -0.11969727 -0.08978427 -0.83030519 -0.08286167 -0.81564909\n",
      "   0.92143629  0.25000883  0.90203579  0.76704666]\n",
      " [-0.97580648  0.42907293 -0.76685355  0.10641305 -0.53946174  0.11725587\n",
      "   0.40701228  0.84448515  0.09824011  0.93094159]\n",
      " [ 0.98500047 -0.05913097 -0.19019918  0.60905772 -0.97462232  0.51927884\n",
      "   0.12602272 -0.80150031 -0.97627526  0.92737517]\n",
      " [ 0.99252796 -0.52726814  0.89908762  0.89219433 -0.54756174  0.53948613\n",
      "   0.63569933  0.24797926 -0.99999371  0.76199531]\n",
      " [ 0.76535854  0.4474374  -0.10911285  0.42924518  0.54111399  0.71304532\n",
      "   0.97266845  0.72975971 -0.83579736  0.23674275]\n",
      " [ 0.26549555  0.59016798  0.8646655  -0.3648548  -0.99690984  0.7294006\n",
      "  -0.57679467  0.99297153  0.76485495  0.95135745]\n",
      " [ 0.02326926  0.9669724  -0.87448117  0.26293077 -0.99797064  0.87671556\n",
      "   0.99975692  0.30378405  0.99069517  0.83173087]\n",
      " [-0.76742931 -0.66052925  0.5717968  -0.99818937  0.12760745  0.73465302\n",
      "   0.7902598   0.98012453 -0.10833825  0.35589632]]\n"
     ]
    }
   ],
   "source": [
    "input_size, hidden_size, output_size = 784, 30, 10\n",
    "B = 16  # batch size\n",
    "\n",
    "nn = NeuralNetwork(784, [30], 10, np.sin)  # ← مقیاس خارج از منابع\n",
    "print(nn.forward(np.random.rand(784, 1)))\n",
    "print(nn.forward(np.random.rand(784, 10))) #seems to work\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf302d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b361cc10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a8ca95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
