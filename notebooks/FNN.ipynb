{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7563926b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a3ebd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self):\n",
    "        self.train, self.valid, self.test = self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Load MNIST train, validation, and test data.\"\"\"\n",
    "        # For notebooks, use relative path to parent directory\n",
    "        data_path = '../mnist.pkl'\n",
    "        with open(data_path, 'rb') as f:\n",
    "            train, valid, test = pickle.load(f, encoding='latin-1')\n",
    "        return train, valid, test\n",
    "    \n",
    "    def get_data_information(self):\n",
    "        \"\"\"Return the shape of the datasets, their data types, unique labels and data range.\"\"\"\n",
    "        return {\n",
    "            'train': (self.train[0].shape, self.train[1].shape),\n",
    "            'train_data_type': type(self.train[0]),\n",
    "            'valid': (self.valid[0].shape, self.valid[1].shape),\n",
    "            'valid_data_type': type(self.valid[0]),\n",
    "            'test': (self.test[0].shape, self.test[1].shape),\n",
    "            'test_data_type': type(self.test[0]),\n",
    "            'unique_labels': set(self.train[1]),\n",
    "            'training_data_range': (self.train[0].min(), self.train[0].max())\n",
    "        }\n",
    "    \n",
    "    def get_train_data(self):\n",
    "        \"\"\"Return training data and labels.\"\"\"\n",
    "        return self.train\n",
    "\n",
    "    def get_valid_data(self):\n",
    "        \"\"\"Return validation data and labels.\"\"\"\n",
    "        return self.valid\n",
    "\n",
    "    def get_test_data(self):\n",
    "        \"\"\"Return test data and labels.\"\"\"\n",
    "        return self.test\n",
    "    \n",
    "    def get_all_data(self):\n",
    "        \"\"\"Return all datasets.\"\"\"\n",
    "        return self.train, self.valid, self.test\n",
    "    \n",
    "    def print_data_summary(self):\n",
    "        \"\"\"Print a summary of the datasets.\"\"\"\n",
    "        info = self.get_data_information()\n",
    "        print(\"Data Summary:\")\n",
    "        print(f\"Training set: {info['train'][0]} samples, Labels: {info['train'][1]}\")\n",
    "        print(f\"Validation set: {info['valid'][0]} samples, Labels: {info['valid'][1]}\")\n",
    "        print(f\"Test set: {info['test'][0]} samples, Labels: {info['test'][1]}\")\n",
    "        print(f\"Unique labels in training set: {info['unique_labels']}\")\n",
    "        print(f\"Training data range: {info['training_data_range']}\")\n",
    "    \n",
    "    def draw_sample(self, data, labels, index=None):\n",
    "        \"\"\"Draw a sample image from the dataset.\n",
    "        \n",
    "        Args:\n",
    "            data: Image data array (samples, height, width) or (samples, pixels)\n",
    "            labels: Label array\n",
    "            index: Index of image to draw (random if None)\n",
    "        \"\"\"\n",
    "        if index is None:\n",
    "            index = np.random.randint(0, len(data))\n",
    "        \n",
    "        image = data[index]\n",
    "        label = labels[index]\n",
    "        \n",
    "        # Reshape if flattened (784 pixels -> 28x28)\n",
    "        if len(image.shape) == 1:\n",
    "            image = image.reshape(28, 28)\n",
    "        \n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.title(f'Label: {label}')\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf332a0",
   "metadata": {},
   "source": [
    "# Data example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4be7620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Summary:\n",
      "Training set: (50000, 784) samples, Labels: (50000,)\n",
      "Validation set: (10000, 784) samples, Labels: (10000,)\n",
      "Test set: (10000, 784) samples, Labels: (10000,)\n",
      "Unique labels in training set: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n",
      "Training data range: (0.0, 0.99609375)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test the DataLoader\n",
    "loader = DataLoader()\n",
    "loader.print_data_summary()\n",
    "train_data, train_labels = loader.get_train_data()\n",
    "loader.draw_sample(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87505dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    \"\"\"Data class for implementing autograd for np.ndarray operations.\"\"\"\n",
    "\n",
    "    def __init__(self, values: np.ndarray, parents=None, grad=False):\n",
    "        self._data = values\n",
    "        self._backward = lambda: None\n",
    "        self._grad = grad\n",
    "        self._num_grad = np.zeros_like(self._data)  # accumulated gradient\n",
    "        self._parents = parents\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self._data)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"grad={self._grad} data={str(self)}\"\n",
    "\n",
    "    def _match_shape(grad, shape):\n",
    "        \"\"\"Sum grad over broadcasted dims so it matches shape.\"\"\"\n",
    "        while len(grad.shape) > len(shape):\n",
    "            grad = grad.sum(axis=0)\n",
    "        for i, (g, s) in enumerate(zip(grad.shape, shape)):\n",
    "            if s == 1 and g > 1:\n",
    "                grad = grad.sum(axis=i, keepdims=True)\n",
    "        return grad\n",
    "\n",
    "    def __add__(self, other):\n",
    "        if isinstance(other, Data):\n",
    "            out = Data(self._data + other._data, parents=(self, other))\n",
    "            def _backward():\n",
    "                self._num_grad += Data._match_shape(out._num_grad, self._data.shape)\n",
    "                other._num_grad += Data._match_shape(out._num_grad, other._data.shape)\n",
    "            out._backward = _backward\n",
    "            return out\n",
    "        out = Data(self._data + other, parents=(self,))\n",
    "        def _backward():\n",
    "            self._num_grad += Data._match_shape(out._num_grad, self._data.shape)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        if isinstance(other, Data):\n",
    "            out = Data(self._data - other._data, parents=(self, other))\n",
    "            def _backward():\n",
    "                self._num_grad += Data._match_shape(out._num_grad, self._data.shape)\n",
    "                other._num_grad += Data._match_shape(-out._num_grad, other._data.shape)\n",
    "            out._backward = _backward\n",
    "            return out\n",
    "        out = Data(self._data - other, parents=(self,))\n",
    "        def _backward():\n",
    "            self._num_grad += Data._match_shape(out._num_grad, self._data.shape)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __rsub__(self, other):\n",
    "        return -self + other\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        if isinstance(other, Data):\n",
    "            out = Data(self._data * other._data, parents=(self, other))\n",
    "            def _backward():\n",
    "                self._num_grad += Data._match_shape(out._num_grad * other._data, self._data.shape)\n",
    "                other._num_grad += Data._match_shape(out._num_grad * self._data, other._data.shape)\n",
    "            out._backward = _backward\n",
    "            return out\n",
    "        out = Data(self._data * other, parents=(self,))\n",
    "        def _backward():\n",
    "            self._num_grad += Data._match_shape(out._num_grad * other, self._data.shape)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "\n",
    "    def __matmul__(self, other):\n",
    "        if isinstance(other, Data):\n",
    "            out = Data(self._data @ other._data, parents=(self, other))\n",
    "            def _backward():\n",
    "                self._num_grad += out._num_grad @ other._data.T\n",
    "                other._num_grad += self._data.T @ out._num_grad\n",
    "            out._backward = _backward\n",
    "            return out\n",
    "        out = Data(self._data @ other, parents=(self,))\n",
    "        def _backward():\n",
    "            self._num_grad += out._num_grad @ other.T\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        if isinstance(other, Data):\n",
    "            out = Data(self._data / other._data, parents=(self, other))\n",
    "            def _backward():\n",
    "                self._num_grad += Data._match_shape(out._num_grad / other._data, self._data.shape)\n",
    "                other._num_grad += Data._match_shape(out._num_grad * (-self._data / (other._data ** 2)),\n",
    "                                                     other._data.shape)\n",
    "            out._backward = _backward\n",
    "            return out\n",
    "        out = Data(self._data / other, parents=(self,))\n",
    "        def _backward():\n",
    "            self._num_grad += Data._match_shape(out._num_grad / other, self._data.shape)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __rtruediv__(self, other):\n",
    "        return other * Data(np.array([1.0])) / self\n",
    "\n",
    "    def __neg__(self):\n",
    "        out = Data(-self._data, parents=(self,))\n",
    "        def _backward():\n",
    "            self._num_grad += Data._match_shape(-out._num_grad, self._data.shape)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"Only supports int/float powers\"\n",
    "        out = Data(self._data ** other, parents=(self,))\n",
    "        def _backward():\n",
    "            self._num_grad += Data._match_shape(out._num_grad * other * self._data ** (other - 1),\n",
    "                                                self._data.shape)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    # --- activations ---\n",
    "    def exp(self):\n",
    "        out = Data(np.exp(self._data), parents=(self,))\n",
    "        def _backward():\n",
    "            self._num_grad += Data._match_shape(np.exp(self._data) * out._num_grad, self._data.shape)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def sigmoid(self):\n",
    "        out = Data(1 / (1 + np.exp(-self._data)), parents=(self,))\n",
    "        def _backward():\n",
    "            self._num_grad += Data._match_shape(out._num_grad * (out._data * (1 - out._data)),\n",
    "                                                self._data.shape)\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "\n",
    "    def backward(self):\n",
    "        #Calculate gradient and if previusly calculated gradients exist, averages\n",
    "\n",
    "        self._nodes = []\n",
    "        visited = set()\n",
    "\n",
    "        def sort(item):  # topological sorting\n",
    "            if not item in visited:\n",
    "                visited.add(item)\n",
    "            if not item._parents:\n",
    "                pass\n",
    "            else:\n",
    "                for parent in item._parents:\n",
    "                    sort(parent)\n",
    "            self._nodes.append(item)\n",
    "\n",
    "        sort(self)\n",
    "        self._num_grad = np.ones_like(self._data, dtype=np.float64)\n",
    "        for item in reversed(self._nodes):\n",
    "            item._backward()\n",
    "        return None\n",
    "\n",
    "    def step(self,step_size):\n",
    "        for item in reversed(self._nodes):\n",
    "            if item._grad:\n",
    "                if item._data.shape[1]==1:\n",
    "                    item._data-=step_size*np.mean(item._num_grad, axis=1).reshape(item._data.shape)\n",
    "                else:\n",
    "                    item._data-=step_size*item._num_grad\n",
    "                item._num_grad=np.zeros_like(item._data)\n",
    "\n",
    "    def mean(self):\n",
    "        out = Data(np.mean(self._data), parents=(self,))\n",
    "        def _backward():\n",
    "            # d(mean)/dx = 1/N for each element\n",
    "            self._num_grad += np.ones_like(self._data) / self._data.size * out._num_grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, input: int, output: int, a_function):\n",
    "        self._weights = Data(np.random.normal(loc=0, scale=0.1, size=(output, input)), grad=True)\n",
    "        self._bias = Data(np.random.normal(loc=0, scale=0.1, size=(output, 1)), grad=True)\n",
    "        \n",
    "        if a_function == 'sigmoid':\n",
    "            self._activation = lambda x: x.sigmoid()\n",
    "        elif a_function == None:\n",
    "            self._activation = lambda x: x\n",
    "        else:\n",
    "            self._activation = a_function\n",
    "\n",
    "    def forward(self, input: np.ndarray):\n",
    "        a = self._weights @ input + self._bias\n",
    "        return self._activation(a)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Layer of {len(self._weights)} neurons\"\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_sizes: list,\n",
    "        output_size: int,\n",
    "        activation_function,\n",
    "        final_activation=True,\n",
    "    ):\n",
    "        if not isinstance(activation_function, list):\n",
    "            activation_function = [\n",
    "                activation_function for _ in range(len(hidden_sizes) + 2)\n",
    "            ]\n",
    "        activation_function[0]=None\n",
    "        sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        self._layers = []\n",
    "        for i in range(len(sizes)-1):\n",
    "            self._layers.append(Layer(sizes[i], sizes[i + 1], activation_function[i]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = x.copy()\n",
    "        for layer in self._layers:\n",
    "            z=layer.forward(z)\n",
    "        return z\n",
    "        # \"\"\"\n",
    "        # X: input with shape (input_size, batch_size) ← mini-batch\n",
    "        # Output: y_hat with shape (output_size, batch_size)\n",
    "        # \"\"\"\n",
    "        # # 1) Add bias neuron to input layer: (input_size+1, batch_size)\n",
    "        # Xb = self._append_bias_row(X)\n",
    "\n",
    "        # # 2) Pre-activation of hidden layer: z1 = W1 · Xb\n",
    "        # z1 = self.W1 @ Xb\n",
    "\n",
    "        # # 3) Hidden layer activation: a1 = sigmoid(z1) → (hidden_size, batch_size)\n",
    "        # a1 = self.sigmoid(z1)\n",
    "\n",
    "        # # 4) Add bias neuron to a1: (hidden_size+1, batch_size)\n",
    "        # a1b = self._append_bias_row(a1)\n",
    "\n",
    "        # # 5) Pre-activation of output layer: z2 = W2 · a1b\n",
    "        # z2 = self.W2 @ a1b\n",
    "\n",
    "        # # 6) Output activation: a2 = sigmoid(z2) → (output_size, batch_size)\n",
    "        # a2 = self.sigmoid(z2)\n",
    "\n",
    "        # # Save for backpropagation\n",
    "        # self.cache = {\n",
    "        #     \"X\": X, \"Xb\": Xb,\n",
    "        #     \"z1\": z1, \"a1\": a1, \"a1b\": a1b,\n",
    "        #     \"z2\": z2, \"a2\": a2\n",
    "        # }\n",
    "        # return a2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "397bcd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, epochs=None, samples=None, batch_size=32, patience=8, lr_factor=0.5, min_lr=1e-5):\n",
    "        data = loader.get_train_data()\n",
    "        self._x_data = data[0]\n",
    "        self._y_data = data[1]\n",
    "        self._batch_size = batch_size\n",
    "        self._epochs = epochs\n",
    "        self._samples = samples if samples is not None else 10000\n",
    "        self._loss_list = []\n",
    "        self._guess = []\n",
    "        self._model = None\n",
    "        self._step_size = 0.1  # initial learning rate\n",
    "        self._patience = patience\n",
    "        self._lr_factor = lr_factor\n",
    "        self._best_val_loss = float('inf')\n",
    "        self._wait = 0\n",
    "        self._min_lr = min_lr\n",
    "        self._current_epoch = 1\n",
    "\n",
    "        if self._epochs:\n",
    "            print(f\"Starting epoch {self._current_epoch}\")\n",
    "\n",
    "    def train(self, network, loss_function=None):\n",
    "        self._loss_list = []\n",
    "        self._guess = []\n",
    "        self._model = network\n",
    "        val_data, val_labels = loader.get_valid_data()\n",
    "        # Use MSE if no loss function is provided\n",
    "        if loss_function is None:\n",
    "            loss_function = lambda y_out, y_correct: (y_out - y_correct) ** 2\n",
    "\n",
    "        for i in range(self._samples):\n",
    "            loss, y_out = self._calculateLoss(self._batch_size, loss_function)\n",
    "            loss.backward()\n",
    "            step_size = self._calculateStepSize(loss, y_out)\n",
    "            loss.step(step_size)\n",
    "            # Validation loss\n",
    "            val_loss = self._validation_loss(val_data, val_labels)\n",
    "            self._loss_list.append(np.linalg.norm(loss._data))\n",
    "            # Regulate learning rate\n",
    "            self._step_size = self.p_regulator(val_loss, step_size)\n",
    "            if self._step_size < self._min_lr:\n",
    "                print(f\"Early stopping at step {i} (learning rate too low)\")\n",
    "                break\n",
    "        return self._model\n",
    "\n",
    "    def _calculateLoss(self, batch_size, loss_function):\n",
    "        idx = np.random.choice(len(self._y_data), batch_size, replace=False)\n",
    "        choosen_x = self._x_data[idx].T\n",
    "        if self._epochs:\n",
    "            self._x_data = np.delete(self._x_data, idx, axis=0)\n",
    "            self._y_data = np.delete(self._y_data, idx, axis=0)\n",
    "            if len(self._y_data) < batch_size:\n",
    "                self._x_data, self._y_data = loader.get_train_data()\n",
    "                self._current_epoch += 1\n",
    "                print(f\"Starting epoch {self._current_epoch}\")\n",
    "                if self._current_epoch >= self._epochs:\n",
    "                    print(\"Reached maximum epochs\")\n",
    "                    exit()\n",
    "        y_out = self._model.forward(choosen_x)\n",
    "        y_correct = np.zeros((y_out._data.shape[0], batch_size))\n",
    "        preds = np.argmax(y_out._data, axis=0)\n",
    "        correct = np.sum(preds == self._y_data[idx])\n",
    "        self._guess.append(correct / batch_size)\n",
    "        y_correct[self._y_data[idx], np.arange(batch_size)] = 1\n",
    "        loss = loss_function(y_out, y_correct).mean()\n",
    "        return loss, y_out\n",
    "\n",
    "    def _calculateStepSize(self, loss, y_out):\n",
    "        grad = loss._num_grad\n",
    "        activation_func = self._model._layers[-1]._activation\n",
    "        nbr_step_test = 100\n",
    "        step_difference = []\n",
    "        for step_test in np.linspace(0.0001, 0.1, nbr_step_test):\n",
    "            w_old = activation_func(y_out)\n",
    "            w_new = activation_func(y_out + step_test * grad)\n",
    "            diff = w_old - w_new\n",
    "            diff_norm = np.linalg.norm(diff._data)\n",
    "            step_difference.append(diff_norm)\n",
    "        max_diff = max(step_difference)\n",
    "        index = step_difference.index(max_diff)\n",
    "        return step_difference[index]\n",
    "\n",
    "    def _validation_loss(self, val_data, val_labels, batch_size=10):\n",
    "        idx = np.random.choice(len(val_labels), batch_size, replace=False)\n",
    "        val_x = val_data[idx].T\n",
    "        y_out = self._model.forward(val_x)\n",
    "        y_correct = np.zeros((y_out._data.shape[0], batch_size))\n",
    "        y_correct[val_labels[idx], np.arange(batch_size)] = 1\n",
    "        val_loss = ((y_out - y_correct) ** 2).mean()._data\n",
    "        return val_loss\n",
    "\n",
    "    def p_regulator(self, val_loss, step_size):\n",
    "        if val_loss < self._best_val_loss:\n",
    "            self._best_val_loss = val_loss\n",
    "            self._wait = 0\n",
    "        else:\n",
    "            self._wait += 1\n",
    "            if self._wait >= self._patience:\n",
    "                step_size = max(step_size * self._lr_factor, self._min_lr)\n",
    "                self._wait = 0\n",
    "        return step_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a15253",
   "metadata": {},
   "source": [
    "Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fd0b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 49540 is out of bounds for axis 0 with size 49540",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[72]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m tran = Trainer(epochs=\u001b[32m10\u001b[39m,batch_size=batch_size)\n\u001b[32m     11\u001b[39m loss_function = \u001b[38;5;28;01mlambda\u001b[39;00m y_out,y_correct: (y_out - y_correct) ** \u001b[32m2\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m nn = \u001b[43mtran\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m plt.plot([i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tran._loss_list))], tran._loss_list)\n\u001b[32m     15\u001b[39m plt.title(\u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, network, loss_function)\u001b[39m\n\u001b[32m     30\u001b[39m     loss_function = \u001b[38;5;28;01mlambda\u001b[39;00m y_out, y_correct: (y_out - y_correct) ** \u001b[32m2\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m._samples):\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     loss, y_out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_calculateLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m     loss.backward()\n\u001b[32m     35\u001b[39m     step_size = \u001b[38;5;28mself\u001b[39m._calculateStepSize(loss, y_out)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 63\u001b[39m, in \u001b[36mTrainer._calculateLoss\u001b[39m\u001b[34m(self, batch_size, loss_function)\u001b[39m\n\u001b[32m     61\u001b[39m y_correct = np.zeros((y_out._data.shape[\u001b[32m0\u001b[39m], batch_size))\n\u001b[32m     62\u001b[39m preds = np.argmax(y_out._data, axis=\u001b[32m0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m correct = np.sum(preds == \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_y_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m     64\u001b[39m \u001b[38;5;28mself\u001b[39m._guess.append(correct / batch_size)\n\u001b[32m     65\u001b[39m y_correct[\u001b[38;5;28mself\u001b[39m._y_data[idx], np.arange(batch_size)] = \u001b[32m1\u001b[39m\n",
      "\u001b[31mIndexError\u001b[39m: index 49540 is out of bounds for axis 0 with size 49540"
     ]
    }
   ],
   "source": [
    "\n",
    "samples = 50000 #10000\n",
    "batch_size = 20 #64\n",
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_size = [30]\n",
    "\n",
    "nn = NeuralNetwork(input_size, hidden_size, output_size, \"sigmoid\")\n",
    "                \n",
    "tran = Trainer(epochs=10, batch_size=batch_size)\n",
    "\n",
    "loss_function = lambda y_out,y_correct: (y_out - y_correct) ** 2\n",
    "nn = tran.train(nn, loss_function)\n",
    "\n",
    "plt.plot([i for i in range(len(tran._loss_list))], tran._loss_list)\n",
    "plt.title(\"loss\")\n",
    "plt.show() \n",
    "acc = []\n",
    "for i in range(0, samples-100, 100):\n",
    "    acc.append(np.sum(tran._guess[i:i+100]))\n",
    "plt.plot([i for i in range(len(acc))], acc)\n",
    "plt.title(\"accuracy (%)\")\n",
    "plt.show()\n",
    "print(\"Final accuracy:\", acc[-1], \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5797cde",
   "metadata": {},
   "source": [
    "## Bitwise representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6dbd22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70f7156e",
   "metadata": {},
   "source": [
    "## ATTACK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ceb4fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# === Test data (MNIST) ===\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m X_test, Y_test = \u001b[43mloader\u001b[49m.get_test_data()          \u001b[38;5;66;03m# or: already have X_test, Y_test\u001b[39;00m\n\u001b[32m      3\u001b[39m X_test = X_test.astype(np.float32, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m X_test.max() > \u001b[32m1.0\u001b[39m:  \u001b[38;5;66;03m# normalize to [0,1] if needed\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'loader' is not defined"
     ]
    }
   ],
   "source": [
    "# === Test data (MNIST) ===\n",
    "X_test, Y_test = loader.get_test_data()          # or: already have X_test, Y_test\n",
    "X_test = X_test.astype(np.float32, copy=False)\n",
    "if X_test.max() > 1.0:  # normalize to [0,1] if needed\n",
    "    X_test /= 255.0\n",
    "\n",
    "# === Minimal helpers ===\n",
    "import numpy as np\n",
    "\n",
    "# Force input to (D, B) to match the model\n",
    "D = nn._layers[0]._weights._data.shape[1]  # e.g., 784 for MNIST\n",
    "def as_DB(X):\n",
    "    \"\"\"Return X as (D,B). Only handles 1D/2D MNIST-style (flat).\"\"\"\n",
    "    X = np.asarray(X, dtype=np.float32)\n",
    "    if X.ndim == 1:\n",
    "        return X.reshape(D, 1)\n",
    "    return X if X.shape[0] == D else X.T\n",
    "\n",
    "def to_indices(Y):\n",
    "    \"\"\"One-hot (C,B) -> indices (B,) or pass-through if already indices.\"\"\"\n",
    "    Y = np.asarray(Y)\n",
    "    return np.argmax(Y, axis=0) if Y.ndim == 2 else Y\n",
    "\n",
    "def predict_idx(M, X):\n",
    "    \"\"\"Argmax over classes.\"\"\"\n",
    "    X_db = as_DB(X)\n",
    "    out = M.forward(Data(X_db))              # Data with shape (C,B)\n",
    "    return np.argmax(out._data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71499ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adapters_autograd import build_grad_x_fn_autograd\n",
    "from attack_fgsm import fgsm_attack\n",
    "\n",
    "# Build dL/dx bridge (square loss), no change to teammates' code\n",
    "grad_x_fn = build_grad_x_fn_autograd(nn, Data)\n",
    "\n",
    "# Craft adversarial examples (untargeted)\n",
    "eps = 0.02\n",
    "x_adv = fgsm_attack(\n",
    "    as_DB(X_test),     # keep shapes consistent\n",
    "    Y_test,\n",
    "    epsilon=eps,\n",
    "    grad_x_fn=grad_x_fn,\n",
    "    clip_min=0.0, clip_max=1.0,\n",
    "    targeted=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d07db7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_idx = to_indices(Y_test)\n",
    "acc_clean = (predict_idx(nn, X_test) == y_true_idx).mean()\n",
    "acc_adv   = (predict_idx(nn, x_adv)  == y_true_idx).mean()\n",
    "print(f\"ε={eps} | Accuracy (clean): {acc_clean*100:.2f}%  | Accuracy (adv): {acc_adv*100:.2f}%\")\n",
    "\n",
    "# Optional: quick sweep\n",
    "eps_list = [0.0, 0.005, 0.01, 0.02, 0.03]\n",
    "print(\"\\nFGSM sweep:\")\n",
    "for e in eps_list:\n",
    "    x_adv_e = fgsm_attack(as_DB(X_test), Y_test, e, grad_x_fn,\n",
    "                          clip_min=0.0, clip_max=1.0, targeted=False)\n",
    "    adv_acc = (predict_idx(nn, x_adv_e) == y_true_idx).mean()\n",
    "    print(f\"  eps={e:>5}: clean={acc_clean*100:6.2f}%  adv={adv_acc*100:6.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
